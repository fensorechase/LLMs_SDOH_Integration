{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GOLD labels. For AHRQ, these are pre-set annotations from the AHRQ database. For NaNDA, these are results of consensus from our 3 human annotators.\n",
    "gold_NaNDA = pd.read_excel(\"../LLM SDOH Classification/annotator_aggreement/total_NaNDA_variable_annotations.xlsx\")\n",
    "gold_AHRQ = pd.read_csv(\"../LLM SDOH Classification/huggingface_dataset_upload/INPUT_AHRQ_tract_2010-2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract LLM predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in NaNDA results:\n",
    "# Get a list of CSV files in the current directory\n",
    "\n",
    "# Define the subdirectory name\n",
    "# Zero shots: \"ZERO_NaNDA\" ZERO_AHRQ\"\n",
    "# One shots: \"ONE_NaNDA\" ONE_AHRQ\"\n",
    "\n",
    "subdirectory = \"ONE_NaNDA\" \n",
    "\n",
    "# Get a list of CSV files in the subdirectory\n",
    "csv_files = [os.path.join(subdirectory, file) for file in os.listdir(subdirectory) if file.endswith('.csv')]\n",
    "# Create an empty dictionary to store dataframes of LLM \"Domain\" predictions\n",
    "dfs_map = {}\n",
    "\n",
    "# Loop through each CSV file and read it into a dataframe\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs_map[file] = df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Only accepts answers with \"(1)\" ... \"(5)\" as correct.\n",
    "def STRICT_map_output_to_extracted_domain(output_value):\n",
    "    output_value = str(output_value)\n",
    "    \n",
    "    if any(value in output_value for value in ['(1)']):\n",
    "        return 1\n",
    "    elif any(value in output_value for value in ['(2)']):\n",
    "        return 2\n",
    "    elif any(value in output_value for value in ['(3)']):\n",
    "        return 3\n",
    "    elif any(value in output_value for value in ['(4)']):\n",
    "        return 4\n",
    "    elif any(value in output_value for value in ['(5)']):\n",
    "        return 5\n",
    "    elif '?' in output_value:\n",
    "        \"\"\"\n",
    "        REFUSAL: \n",
    "\n",
    "        '?' in response: catches \n",
    "        (i) a simple response of only \"?\", and \n",
    "        (ii) responses asking for more context via a response question: e.g.,) \"[/INST] I'm unable to determine the domain of the variable \"count_emp_491\" based on its name alone. Could you please provide more context or information about what this variable represents?\"\n",
    "        \"\"\"\n",
    "        return '?'\n",
    "    else:\n",
    "        # To catch PROMPT NON-ADHEARANCE \n",
    "        return 'UNKNOWN'\n",
    "\n",
    "# For FLAN-T5 models: Define a function to map output values to extracted_domain values\n",
    "def FLAN_T5_map_output_to_extracted_domain(output_value):\n",
    "    output_value = str(output_value)\n",
    "    if any(value in output_value for value in ['unable']):\n",
    "        # To catch REFUSAL.\n",
    "        return '?'\n",
    "    elif any(value in output_value for value in ['1']):\n",
    "        return 1\n",
    "    elif any(value in output_value for value in ['2']):\n",
    "        return 2\n",
    "    elif any(value in output_value for value in ['3']):\n",
    "        return 3\n",
    "    elif any(value in output_value for value in ['4']):\n",
    "        return 4\n",
    "    elif any(value in output_value for value in ['5']):\n",
    "        return 5\n",
    "    elif '?' in output_value:\n",
    "        \"\"\"\n",
    "        REFUSAL: \n",
    "\n",
    "        '?' in response: catches \n",
    "        (i) a simple response of only \"?\", and \n",
    "        (ii) responses asking for more context via a response question: e.g.,) \"[/INST] I'm unable to determine the domain of the variable \"count_emp_491\" based on its name alone. Could you please provide more context or information about what this variable represents?\"\n",
    "        \"\"\"\n",
    "        return '?'\n",
    "    else:\n",
    "        # To catch PROMPT NON-ADHEARANCE \n",
    "        return 'UNKNOWN'\n",
    "\n",
    "\n",
    "# Gemma helper: Gemma models tend to return '1' (for example) nested within the response. \n",
    "# We CANNOT say 'correct' if there's a '1' anywhere in the response because the variable name may have a '1'.\n",
    "# This function returns True iff the '1' does not have any alphnumeric neighbors, as it would within a variable name.\n",
    "def contains_lonely_digit(s, digit):\n",
    "    regex = r'\\b{}\\b'.format(digit)\n",
    "    return bool(re.search(regex, s))\n",
    "\n",
    "# Define a function to map output values to extracted_domain values\n",
    "def map_output_to_extracted_domain(output_value):\n",
    "    # First, cast to string.\n",
    "    \n",
    "    output_value = str(output_value)\n",
    "    if any(value in output_value for value in ['unable']):\n",
    "        # To catch REFUSAL.\n",
    "        return '?'\n",
    "    elif any(value in output_value for value in ['(1)', 'social_and_community_context', 'Social and Community Context']) or contains_lonely_digit(output_value, 1):\n",
    "        return 1\n",
    "    elif any(value in output_value for value in ['(2)', 'economic_stability', 'Economic Stability']) or contains_lonely_digit(output_value, 2):\n",
    "        return 2\n",
    "    elif any(value in output_value for value in ['(3)', 'education_access_and_quality', 'Education Access and Quality']) or contains_lonely_digit(output_value, 3):\n",
    "        return 3\n",
    "    elif any(value in output_value for value in ['(4)', 'neighborhood_and_built_environment', 'Neighborhood and Built Environment']) or contains_lonely_digit(output_value, 4):\n",
    "        return 4\n",
    "    elif any(value in output_value for value in ['(5)', 'health_care_and_quality', 'Healthcare Access and Quality']) or contains_lonely_digit(output_value, 5):\n",
    "        return 5\n",
    "    elif '?' in output_value:\n",
    "        \"\"\"\n",
    "        REFUSAL: \n",
    "\n",
    "        '?' in response: catches \n",
    "        (i) a simple response of only \"?\", and \n",
    "        (ii) responses asking for more context via a response question: e.g.,) \"[/INST] I'm unable to determine the domain of the variable \"count_emp_491\" based on its name alone. Could you please provide more context or information about what this variable represents?\"\n",
    "        \"\"\"\n",
    "        return '?'\n",
    "    else:\n",
    "        # To catch PROMPT NON-ADHEARANCE \n",
    "        return 'UNKNOWN'\n",
    "\n",
    "# Loop through each dataframe in the dictionary and create extracted_domain column\n",
    "for filename, df in dfs_map.items():\n",
    "    # Note: Because of different output repsonse formats, use Flan-T5 models RAW responses (the 'raw_output' column).\n",
    "    # ... for all other models, we use the 'output' column. The 'output' column strips the prompt prefix from the response.\n",
    "    # Imporantly: This is a trivial text parsing correction we took, not a limitation of Flan-T5 models.\n",
    "\n",
    "    if any(value in filename for value in ['flan', 'Flan']):\n",
    "        df['extracted_domain'] = df['raw_output'].apply(FLAN_T5_map_output_to_extracted_domain)\n",
    "        # print(\"APPLIED\")\n",
    "    else: # Evalute on-FLAN models:\n",
    "        df['extracted_domain'] = df['output'].apply(map_output_to_extracted_domain)\n",
    "    print(df['extracted_domain'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy compared to gold labels (gold_NaNDA, gold_AHRQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a list to store the rates\n",
    "fnames = []\n",
    "exact_match_rates = []\n",
    "f1_scores_macro = []\n",
    "f1_scores_micro = []\n",
    "qmark_rates = []\n",
    "garbage_rates = [] # % of model responses that didn't include domains (1)-(5) or ?\n",
    "\n",
    "\n",
    "# Iterate over each dataframe in dfs_map\n",
    "for filename, df in dfs_map.items():\n",
    "\n",
    "    fnames.append(filename)\n",
    "    # Calculate the total number of rows in the dataframe\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # gold_labels = gold_AHRQ['domain'].astype(str)\n",
    "    #merged_preds_labels = None\n",
    "\n",
    "    if 'NaNDA' in subdirectory:\n",
    "        # gold_labels = gold_NaNDA['RESOLVED_CONSENSUS'].astype(str)\n",
    "        # Merge on 'variable_name'\n",
    "        merged_preds_labels = pd.merge(df, gold_NaNDA, on='variable_name')\n",
    "        predicted_labels = merged_preds_labels['extracted_domain'].astype(str)\n",
    "        gold_labels = merged_preds_labels['RESOLVED_CONSENSUS'].astype(str) \n",
    "    elif 'AHRQ' in subdirectory:\n",
    "        merged_preds_labels = pd.merge(df, gold_AHRQ, on=['variable_name', 'domain'])\n",
    "        predicted_labels = merged_preds_labels['extracted_domain'].astype(str)\n",
    "        gold_labels = merged_preds_labels['domain'].astype(str)\n",
    "\n",
    "\n",
    "    # Calculate the number of exact string matches in the 'output' column\n",
    "    exact_matches = (predicted_labels == gold_labels).sum()\n",
    "    \n",
    "    # Calculate the percentage of exact string matches\n",
    "    match_rate = (exact_matches / total_rows) * 100\n",
    "    exact_match_rates.append(match_rate)\n",
    "\n",
    "    # Track poor performances: \n",
    "    pct_qmark = (predicted_labels == '?').sum() / total_rows\n",
    "    qmark_rates.append(pct_qmark)\n",
    "    pct_garbage = (predicted_labels == 'UNKNOWN').sum() / total_rows\n",
    "    garbage_rates.append(pct_garbage)\n",
    "    \n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_mac = f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    f1_micro = f1_score(gold_labels, predicted_labels, average='micro') # 'weighted'\n",
    "    f1_scores_macro.append(f1_mac)\n",
    "    f1_scores_micro.append(f1_micro)\n",
    "\n",
    "# Print the list of rates\n",
    "fnames = [s.split(\"/\", 1)[1] for s in fnames]\n",
    "print(fnames)\n",
    "print(exact_match_rates)\n",
    "print(\"MAX ACCURACY: \", max(exact_match_rates))\n",
    "idx_max_acc = exact_match_rates.index(max(exact_match_rates))\n",
    "print(\"FNAME: \", fnames[idx_max_acc])\n",
    "\n",
    "print(f1_scores_macro)\n",
    "print(\"MAX F1-Macro: \", max(f1_scores_macro))\n",
    "idx_max_acc = f1_scores_macro.index(max(f1_scores_macro))\n",
    "print(\"FNAME: \", fnames[idx_max_acc])\n",
    "\n",
    "print(f1_scores_micro)\n",
    "print(\"MAX F1-Micro: \", max(f1_scores_micro))\n",
    "idx_max_acc = f1_scores_micro.index(max(f1_scores_micro))\n",
    "print(\"FNAME: \", fnames[idx_max_acc])\n",
    "\n",
    "print(\"Q MARK RATES: \", qmark_rates)\n",
    "print(\"GARBAGE RATES: \", garbage_rates)\n",
    "\n",
    "# Get max values:\n",
    "# idx_max_acc = exact_match_rates.index(max(exact_match_rates))\n",
    "# print(fnames[idx_max_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom sorting key function to output prompt ablation results in ascending order.\n",
    "def custom_sort_key(x):\n",
    "    if x.startswith('abc'):\n",
    "        return 7\n",
    "    elif x.startswith('a') and not x.startswith('ab') and not x.startswith('ac'):\n",
    "        return 1\n",
    "    elif x.startswith('b') and not x.startswith('bc'):\n",
    "        return 2\n",
    "    elif x.startswith('c'):\n",
    "        return 3\n",
    "    elif x.startswith('ab') and not x.startswith('abc'):\n",
    "        return 4\n",
    "    elif x.startswith('ac'):\n",
    "        return 5\n",
    "    elif x.startswith('bc'):\n",
    "        return 6\n",
    "    else:\n",
    "        return 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display LLM inference results as LaTeX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip models + performances.\n",
    "\n",
    "nanda_zero_performance = pd.DataFrame({'Model': fnames, 'Refusal': qmark_rates}) # 'F1-micro': f1_scores_micro\n",
    "# nanda_zero_performance = pd.DataFrame({'Model': fnames, 'Accuracy': exact_match_rates}) # ACCURACY\n",
    "# nanda_zero_performance = pd.DataFrame({'Model': fnames, 'F1-macro': f1_scores_macro}) # Macro F1\n",
    "# nanda_zero_performance = pd.DataFrame({'Model': fnames, '? Rate': qmark_rates, 'Prompt Non-adherence': garbage_rates}) # NON-ADHERENCE\n",
    "# nanda_zero_performance = pd.DataFrame({'Model': fnames, 'Prompt Non-adherence': garbage_rates}) # REFUSAL\n",
    "\n",
    "\n",
    "# Filter for model:\n",
    "# FOR ROW ORDER:\n",
    "z_shot_nanda_names = ['llama7b', 'llama13b', 'llama70b', 'gemma-2b', 'gemma7b', 'mistral7bv1', 'mistral7bv2', 'flant5xl', 'flant5xxl']\n",
    "z_shot_AHRQ_names = ['llama7b', 'llama13b', 'Llama-2-70b', 'gemma-2b', 'gemma7b', 'mistral7bv1', 'mistral7bv2', 'flant5xl']\n",
    "o_shot_nanda_names = ['Llama-2-7b', 'Llama-2-13b', 'Llama-2-70b', 'gemma-2b', 'gemma-7b', 'Mistral-7B-Instruct-v0.1', 'Mistral-7B-Instruct-v0.2', 'flan-t5-xl', 'flan-t5-xxl']\n",
    "o_shot_AHRQ_names = []\n",
    "filtered_df = nanda_zero_performance[nanda_zero_performance['Model'].str.contains(o_shot_nanda_names[8])] # Mistral-7B-Instruct-v0.2\n",
    "\n",
    "# filtered_df.sort_values(by='Model')\n",
    "print_df = filtered_df.iloc[filtered_df['Model'].map(custom_sort_key).argsort()].T\n",
    "\n",
    "\n",
    "print(print_df.to_latex(index=False,\n",
    "                  float_format=\"{:.3f}\".format))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
